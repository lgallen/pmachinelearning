---
title: "Predictive Function For Weight Lifting Exercise Dataset"
author: "Lucas Allen"
date: "Friday, August 22, 2014"
output: html_document
---
## Project Description

This is the final project for Practical Machine Learning, course 8 in the Johns Hopkins Data Science Specialization on Coursera. The final project asked students to download a weightlifting data set that included a number of quantifiers as to how the weightlifting activity was performed, primarily via activity sensors (accelerometers). Participants in performed weightlifting correctly and incorrectly 5 different ways. These outcomes were recorded in the "classe" variable. 

The goal of the project was to predict the manner in which participants did the exercise. A training set was provided, and then 20 additional cases were provided in a testing set for grading purposes.

## Examining and Cleaning the Data Set

Begin by downloading the training and testing data sets, and reading them into memory.

```{r, cache=TRUE}
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",dest="pml-training.csv")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",dest="pml-testing.csv")

training<-read.csv("pml-training.csv")
testing<-read.csv("pml-testing.csv")
dim(training)
```

A good place to start is with a quick visual examination of the data set. 
```{r,cache=TRUE}
head(training)
```
From looking at the head of the data set, one would already start to suspect that there may be columns that are primarily empty columns or full of NA's. This can be further verified with the psych package's describe function.
```{r,cache=TRUE}
library(psych)
desTraining<-describe(training)
desTraining[,1:4]
```
Here we can see that, as suspected, there are numerous columns that have little to no data in them. Most of them seemed to be summary statistics rather than individual measurements. Since these are unlikely to be strong predictors for classe, I now moved towards creating a new, clean data set that I called "cleanTraining". 

Most of the features that seemed irrelevant to a prediction algorithm for classe due to lack of data had text in common. For example, "amplitude" appeared in some, "kurtosis", "max", "min", and so for. By examining the data from "describe" along with my visual inspection of the training set, I settled on the following grep and clean. To be certain I had not accidentally grepped any variables that contained a significant number of data values, I also examined a summary of the columns before removing those columns from cleanTraining. All of them had greater than 19000 missing values in a data set with 19622 rows.

```{r, cache=TRUE}
cleanTraining<-training
cleanCols<-grep("skewness|kurtosis|amplitude|min|max|avg|stddev|var",colnames(cleanTraining))
summary(cleanTraining[,minmaxCols])
# All have >19000 missing values.
cleanTraining<-cleanTraining[,-cleanCols]
```

Additionally, there is no reason to believe that the first 5 columns would be of any value as predictors in this scenario. Column 1 is and index column. Column 2 is the user name, and columns 3-5 are time stamps, none of which should have any impact on how a participant performs an exercise. As such, I also removed these columns.

```{r, cache=TRUE}
cleanTraining<-cleanTraining[,-(1:5)]
```

## Creating a Prediction Model

At this point, the data set is clean. However, what we have called the "training set" to this point is needs to be further divided into a new training and new testing set for cross validation purposes since the "testing set" we are using will only be used for grading at the end of the project and really should not be used for cross validation. Below, I have loaded the caret package and divide 60% of the data into a trainingNew data set and 40% into a testingNew.

```{r, cache=TRUE}
library(caret)
inTrain<-createDataPartition(y=cleanTraining$classe,p=.6,list=FALSE)
trainingNew<-cleanTraining[inTrain,]
testingNew<-cleanTraining[-inTrain,]

dim(trainingNew)
dim(testingNew)
```

I chose to use the random forest method to generate my predictions. First, I loaded the randomForest library and set my seed. When I generated my model, I set importance to TRUE because I wanted to take a closer look later at which variables were of the most importance to the model with a variable importance plot. 

```{r, cache=TRUE}
library(randomForest)
set.seed(1234)
modFit<-randomForest(classe~.,data=trainingNew,importance=TRUE)
```

At this point, I could examine how well the model generated by the random forest did by creating a confusion matrix.

```{r, cache=TRUE}
predictionsTraining<-predict(modFit,trainingNew)
confusionMatrix(predictionsTraining,trainingNew$classe)
```

After generating predictions for the training set, the confusion matrix revealed perfect prediction accuracy (Accuracy=1) with respect to the training set. 

## Estimating Error with Cross Validation

While this is encouraging, there is always going to be some overfitting that occurs with the training set, and this is likely reflected in this number. Consequently, I now ran predictions and found a confusion matrix for the cross validation test set that I had created.

```{r,cache=TRUE}
predictionsTesting<-predict(modFit,testingNew)
confusionMatrix(predictionsTesting,testingNew$classe)
```

Fortunately, the results remained very encouraging. The confusion matrix with the predictions from the "testingNew" still showed accuracy of .996, a very high prediction accuracy in this scenario. 

In order to get a better sense of which variables were of the greatest importance in the model, I used the varImpPlot function.

```{r}
library(randomForest)
varImpPlot(modFit)
```

Because I used importance=TRUE on my randomForest call earlier, the varImpPlot reveals both the meanDecreaseAccuracy and the meanDecreaseGini. Between the two plots, there many differences over which predictors are the least significant features. However, meanDecreaseAccuracy and meanDecreaseGini plots are mostly in agreement about the most important predictors. We can note that both plots ranked "magnet_dumbbell_z", "magnet_dumbbell_y", "num_window", "pitch_belt", "pitch_forearm", "roll_belt", and "yaw_belt" as the 7 most important features, albeit in different orders.

## Making the Required Predictions

Having cross validated against "testingNew" and obtained a high degree of accuracy, and having examined the most important features with varImpPlot, I began the process of making predictions for the assignment submission. I quickly discovered that making even a simple prediction on a single row resulted in an error message.

A little bit of further research led me to realize that the problem was that when using a model generated by "randomForest", it is critical to make sure that any factor variables have the same number of levels in the data set that you are making predictions with as they did in the training set. In this case, the variable "new_window" had two levels, "no" and "yes" in the training set, but has only "no" in the testing set that I was using for predictions. This can be addressed easily as follows.

```{r, cache=TRUE}
testing$new_window<-factor(testing$new_window,levels=c("no","yes"))
```

From here, it was a simple matter of storing my predictions for grading purposes in a new variable that I called "finalPredictions".

```{r,cache=TRUE}
finalPredictions<-predict(modFit,testing)
```

All predictions stored in this vector were later verified correct.